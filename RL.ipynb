{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2bc7128",
   "metadata": {},
   "source": [
    "# L7: Online RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ba927b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Warning control\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f50dcd5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import TrainingArguments, AutoTokenizer, AutoModelForCausalLM\n",
    "from trl import GRPOTrainer, GRPOConfig\n",
    "from datasets import load_dataset, Dataset\n",
    "from helper import generate_responses, test_model_with_questions, load_model_and_tokenizer\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88ea902",
   "metadata": {},
   "source": [
    "## Prepare for evaluation dataset for Math: GSM8K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f5a8b9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "USE_GPU = False\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are a helpful assistant that solves problems step-by-step. \"\n",
    "    \"Always include the final numeric answer inside \\\\boxed{}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e417f948",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def reward_func(completions, ground_truth, **kwargs):\n",
    "    # Regular expression to capture content inside \\boxed{}\n",
    "    matches = [re.search(r\"\\\\boxed\\{(.*?)\\}\", completion[0]['content']) for completion in completions]\n",
    "    contents = [match.group(1) if match else \"\" for match in matches]\n",
    "    # Reward 1 if the content is the same as the ground truth, 0 otherwise\n",
    "    return [1.0 if c == gt else 0.0 for c, gt in zip(contents, ground_truth)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692f640d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "sample_pred = [[{\"role\": \"assistant\", \n",
    "                 \"content\": r\"...Calculating the answer. \\boxed{72}\"}]]\n",
    "ground_truth = [\"72\"]\n",
    "reward = reward_func(sample_pred, ground_truth)\n",
    "print(f\"Positive Sample Reward: {reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dc6448",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "sample_pred = [[{\"role\": \"assistant\", \n",
    "                 \"content\": r\"...Calculating the answer \\boxed{71}\"}]]\n",
    "ground_truth = [\"72\"]\n",
    "reward = reward_func(sample_pred, ground_truth)\n",
    "print(f\"Negative Sample Reward: {reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f8d124",
   "metadata": {},
   "source": [
    "## Load the Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5455636",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "data_num = 5\n",
    "eval_dataset = load_dataset(\"openai/gsm8k\", \"main\")[\"test\"].select(range(data_num))\n",
    "sample_df = eval_dataset.to_pandas()\n",
    "display(sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8aeebd6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def post_processing(example):\n",
    "    match = re.search(r\"####\\s*(-?\\d+)\", example[\"answer\"])\n",
    "    example[\"ground_truth\"] = match.group(1) if match else None\n",
    "    example[\"prompt\"] = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": example[\"question\"]}\n",
    "    ]\n",
    "    return example\n",
    "eval_dataset = eval_dataset.map(post_processing).remove_columns([\"question\", \"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e9925c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "sample_df = eval_dataset.select(range(5)).to_pandas()\n",
    "display(sample_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b152d00",
   "metadata": {},
   "source": [
    "## Load the model and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04128032",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model, tokenizer = load_model_and_tokenizer(\"./models/Qwen/Qwen2.5-0.5B-Instruct\", USE_GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69076c04",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Store predictions and ground truths\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "for example in tqdm(eval_dataset):\n",
    "    input_prompt = example[\"prompt\"]\n",
    "    ground_truth = example[\"ground_truth\"]\n",
    "    # Run the model to generate an answer\n",
    "    with torch.no_grad():\n",
    "        response = generate_responses(model, tokenizer, \n",
    "                                      full_message = input_prompt) \n",
    "    all_preds.append([{\"role\": \"assistant\", \"content\": response}])\n",
    "    all_labels.append(ground_truth)\n",
    "    print(response)\n",
    "    print(\"Ground truth: \", ground_truth)\n",
    "\n",
    "# 3. Evaluate using reward_func\n",
    "rewards = reward_func(all_preds, all_labels)\n",
    "\n",
    "# 4. Report accuracy\n",
    "accuracy = sum(rewards) / len(rewards)\n",
    "print(f\"Evaluation Accuracy: {accuracy:.2%}\")\n",
    "del model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2648843d",
   "metadata": {},
   "source": [
    "## Loading the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf4f803",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"openai/gsm8k\", \"main\")\n",
    "train_dataset = dataset[\"train\"]\n",
    " \n",
    "# Apply to dataset\n",
    "train_dataset = train_dataset.map(post_processing)\n",
    "train_dataset = train_dataset.remove_columns([\"question\", \"answer\"])\n",
    "if not USE_GPU:\n",
    "    train_dataset = train_dataset.select(range(10))\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d85d853",
   "metadata": {},
   "source": [
    "## GRPO Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9c1155",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "config = GRPOConfig(\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_generations=4, # Can set as high as 64 or 128\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=5e-6,\n",
    "    logging_steps=2,\n",
    "    no_cuda= not USE_GPU     # keeps the whole run on CPU, incl. MPS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df8afff",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "model, tokenizer = load_model_and_tokenizer(\"./models/HuggingFaceTB/SmolLM2-135M-Instruct\", USE_GPU)\n",
    "\n",
    "grpo_trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    args=config,\n",
    "    reward_funcs=reward_func,\n",
    "    train_dataset=train_dataset\n",
    ")\n",
    "\n",
    "grpo_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100747e0",
   "metadata": {},
   "source": [
    "## Results of the fully trained Qwen model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a651946",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "fully_trained_qwen = True\n",
    "if fully_trained_qwen:\n",
    "    model, tokenizer = load_model_and_tokenizer(\"./models/banghua/Qwen2.5-0.5B-GRPO\", \n",
    "                                            USE_GPU)\n",
    "else:\n",
    "    model = grpo_trainer.model\n",
    "\n",
    "# Store predictions and ground truths\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "for example in tqdm(eval_dataset):\n",
    "    input_prompt = example[\"prompt\"]\n",
    "    ground_truth = example[\"ground_truth\"]\n",
    "    # Run the model to generate an answer\n",
    "    with torch.no_grad():\n",
    "        response = generate_responses(model, tokenizer, \n",
    "                                      full_message = input_prompt) \n",
    "    all_preds.append([{\"role\": \"assistant\", \"content\": response}])\n",
    "    all_labels.append(ground_truth)\n",
    "    print(response)\n",
    "    print(\"Ground truth: \", ground_truth)\n",
    "\n",
    "# 3. Evaluate using reward_func\n",
    "rewards = reward_func(all_preds, all_labels)\n",
    "\n",
    "# 4. Report accuracy\n",
    "accuracy = sum(rewards) / len(rewards)\n",
    "print(f\"Evaluation Accuracy: {accuracy:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
